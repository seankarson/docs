<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/concept.dtd">
<concept id="how_pr_ha_works">
	<title>Understanding High Availability for Partitioned Regions</title>
	<shortdesc> With high availability, each member that hosts data for the partitioned region gets
		some primary copies and some redundant (secondary) copies. </shortdesc>
	<conbody>
		<section id="section_04FDCC6C2130496F8B33B9DF5CDED362">
			<p> </p>
			<p>With redundancy, if one member fails, operations continue on the partitioned region
				with no interruption of service: <ul id="ul_7A221AE2AAD84FFF8F9C1B30F1460AD4">
					<li id="li_060C08293ACC4CB9BE161FC61E7B9244">If the member hosting the primary
						copy is lost, <keyword keyref="product_name"/> makes a secondary copy the
						primary. This might cause a temporary loss of redundancy, but not a loss of
						data. </li>
					<li id="li_BB3BAB83CA8F486CA8ED6289808EEDF3">Whenever there are not enough
						secondary copies to satisfy redundancy, the system works to recover
						redundancy by assigning another member as secondary and copying the data to
						it. </li>
				</ul>
				<note>You can still lose cached data when you are using redundancy if enough members
					go down in a short enough time span. </note>You can configure how the system
				works to recover redundancy when it is not satisfied. You can configure recovery to
				take place immediately or, if you want to give replacement members a chance to start
				up, you can configure a wait period. Redundancy recovery is also automatically
				attempted during any partitioned data rebalancing operation. Use the
					<codeph>gemfire.MAX_PARALLEL_BUCKET_RECOVERIES</codeph> system property to
				configure the maximum number of buckets that are recovered in parallel. By default,
				up to 8 buckets are recovered in parallel any time the system attempts to recover
				redundancy.</p>
			<p>Without redundancy, the loss of any of the region's data stores causes the loss of
				some of the region's cached data. Generally, you should not use redundancy when your
				applications can directly read from another data source, or when write performance
				is more important than read performance. </p>
		</section>
		<section id="section_7045530D601F4C65A062B5FDD0DD9206">
			<title>Controlling Where Your Primaries and Secondaries Reside</title>
			<p>By default, <keyword keyref="product_name"/> places your primary and secondary data
				copies for you, avoiding placement of two copies on the same physical machine. If
				there are not enough machines to keep different copies separate, <keyword
					keyref="product_name"/> places copies on the same physical machine. You can
				change this behavior, so <keyword keyref="product_name"/> only places copies on
				separate machines. </p>
			<p> You can also control which members store your primary and secondary data copies.
					<keyword keyref="product_name"/> provides two options: <ul
					id="ul_3C415128F5B34D688097F7752CB9C650">
					<li id="li_6DB269998B794F49A266990E7AB24096"><b>Fixed custom partitioning</b>.
						This option is set for the region. Fixed partitioning gives you absolute
						control over where your region data is hosted. With fixed partitioning, you
						provide <keyword keyref="product_name"/> with the code that specifies the
						bucket and data store for each data entry in the region. When you use this
						option with redundancy, you specify the primary and secondary data stores.
						Fixed partitioning does not participate in rebalancing because all bucket
						locations are fixed by you. </li>
					<li id="li_58E971520C824FD08805B59643B711C1"><b>Redundancy zones</b>. This
						option is set at the member level. Redundancy zones let you separate primary
						and secondary copies by member groups, or zones. You assign each data host
						to a zone. Then <keyword keyref="product_name"/> places redundant copies in
						different redundancy zones, the same as it places redundant copies on
						different physical machines. You can use this to split data copies across
						different machine racks or networks, This option allows you to add members
						on the fly and use rebalancing to redistribute the data load, with redundant
						data maintained in separate zones. When you use redundancy zones, <keyword
							keyref="product_name"/> will not place two copies of the data in the
						same zone, so make sure you have enough zones. </li>
				</ul>
			</p>
		</section>
		<section id="section_87A2429B6277497184926E08E64B81C6">
			<title>Running Processes in Virtual Machines</title>
			<p>By default, <keyword keyref="product_name"/> stores redundant copies on different
				machines. When you run your processes in virtual machines, the normal view of the
				machine becomes the VM and not the physical machine. If you run multiple VMs on the
				same physical machine, you could end up storing partitioned region primary buckets
				in separate VMs, but on the same physical machine as your secondaries. If the
				physical machine fails, you can lose data. When you run in VMs, you can configure
					<keyword keyref="product_name"/> to identify the physical machine and store
				redundant copies on different physical machines. </p>
		</section>
		<section id="section_CAB9440BABD6484D99525766E937CB55">
			<title>Reads and Writes in Highly-Available Partitioned Regions</title>
			<p><keyword keyref="product_name"/> treats reads and writes differently in
				highly-available partitioned regions than in other regions because the data is
				available in multiple members: <ul id="ul_92f08af3-9cb5-49dc-a6af-13cc5741bbfb">
					<li id="li_5D4674FB72D645599074E26655DB4EFE">Write operations (like
							<codeph>put</codeph> and <codeph>create</codeph>) go to the primary for
						the data keys and then are distributed synchronously to the redundant
						copies. Events are sent to the members configured with
							<codeph>subscription-attributes</codeph>
						<codeph>interest-policy</codeph> set to <codeph>all</codeph>. </li>
					<li id="li_42CB83C691EA49CBB31005264CE3F6F4">Read operations go to any member
						holding a copy of the data, with the local cache favored, so a read
						intensive system can scale much better and handle higher loads. </li>
				</ul>
			</p>
			<p>In this figure, M1 is reading W, Y, and Z. It gets W directly from its local copy.
				Since it doesn't have a local copy of Y or Z, it goes to a cache that does, picking
				the source cache at random. </p>
			<p>
				<image placement="break" id="image_574D1A1E641944D2A2DE68C4618D84B4"
					href="../../images_svg/partitioned_data_HA.svg"/>
			</p>
		</section>
	</conbody>
</concept>
